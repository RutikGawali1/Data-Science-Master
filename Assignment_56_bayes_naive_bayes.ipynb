{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Bayes' Theorem and Naive Bayes Classification\n", "This notebook covers key questions and an assignment on Bayes' theorem and Naive Bayes classification."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Q1: What is Bayes' theorem?\n", "Bayes' theorem describes the probability of an event based on prior knowledge of related conditions. It is used in various applications such as spam filtering, medical diagnosis, and machine learning."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Q2: Formula for Bayes' theorem\n", "The formula for Bayes' theorem is:\n", "\\[\n", "P(A | B) = \\frac{P(B | A) P(A)}{P(B)}\n", "\\]\n", "where:\n", "- \\( P(A | B) \\) is the posterior probability of event A given B\n", "- \\( P(B | A) \\) is the likelihood of event B given A\n", "- \\( P(A) \\) is the prior probability of event A\n", "- \\( P(B) \\) is the probability of event B"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Q3: How is Bayes' theorem used in practice?\n", "Bayes' theorem is used in applications such as:\n", "- **Spam filtering**: Determining whether an email is spam\n", "- **Medical diagnosis**: Estimating the probability of a disease given test results\n", "- **Machine learning**: Probabilistic classification methods like Naive Bayes"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Q4: Relationship between Bayes' theorem and conditional probability\n", "Bayes' theorem is derived from the definition of conditional probability. It helps update prior beliefs based on new evidence."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Q5: Choosing the Right Naive Bayes Classifier\n", "- **Gaussian Naive Bayes**: Used for continuous numerical data (e.g., heights, weights).\n", "- **Multinomial Naive Bayes**: Used for discrete counts (e.g., word frequencies in text classification).\n", "- **Bernoulli Naive Bayes**: Used for binary features (e.g., presence or absence of a word in spam filtering)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Q6: Naive Bayes Classification Assignment\n", "We use the given frequency table to classify a new instance \\( X_1 = 3, X_2 = 4 \\) using Naive Bayes."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "# Given frequency table\n", "class_counts = {'A': 10, 'B': 5}  # Total counts per class\n", "feature_counts = {\n", "    'A': {'X1_3': 4, 'X2_4': 3},\n", "    'B': {'X1_3': 1, 'X2_4': 3}\n", "}\n", "\n", "# Calculate probabilities using Laplace smoothing\n", "total_instances = sum(class_counts.values())\n", "prior_A = class_counts['A'] / total_instances\n", "prior_B = class_counts['B'] / total_instances\n", "\n", "likelihood_X1_3_A = feature_counts['A']['X1_3'] / class_counts['A']\n", "likelihood_X2_4_A = feature_counts['A']['X2_4'] / class_counts['A']\n", "likelihood_X1_3_B = feature_counts['B']['X1_3'] / class_counts['B']\n", "likelihood_X2_4_B = feature_counts['B']['X2_4'] / class_counts['B']\n", "\n", "# Compute posterior probabilities\n", "posterior_A = likelihood_X1_3_A * likelihood_X2_4_A * prior_A\n", "posterior_B = likelihood_X1_3_B * likelihood_X2_4_B * prior_B\n", "\n", "# Normalize probabilities\n", "total_posterior = posterior_A + posterior_B\n", "prob_A = posterior_A / total_posterior\n", "prob_B = posterior_B / total_posterior\n", "\n", "# Predict the class\n", "predicted_class = 'A' if prob_A > prob_B else 'B'\n", "print(f'Class A Probability: {prob_A:.4f}')\n", "print(f'Class B Probability: {prob_B:.4f}')\n", "print(f'Predicted Class: {predicted_class}')"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 4}