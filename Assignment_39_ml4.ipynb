{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its  application.\n",
        "Ans -\n",
        "\n",
        "Min-Max scaling is a data normalization technique that rescales the values of each feature in a dataset to a range of 0 to 1. This is done by subtracting the minimum value of each feature from all of its values, and then dividing by the range of the feature.\n",
        "\n",
        "Min-Max scaling is used in data preprocessing to ensure that all features have a similar scale. This is important for many machine learning algorithms, as they can be sensitive to the scale of the features. For example, an algorithm that uses Euclidean distance as a similarity measure will be more sensitive to features with larger values.\n",
        "\n",
        "To illustrate the application of Min-Max scaling, consider the following dataset of house prices:\n",
        "- Feature | Value\n",
        "------- | --------\n",
        "Sq. ft. | 1000\n",
        "Bedrooms | 3\n",
        "Bathrooms | 2\n",
        "Price | \\$500,000\n",
        "\n",
        "The minimum value of the Sq. ft. feature is 1000, and the maximum value is 2000. The minimum value of the Price feature is $500,000, and the maximum value is $1,000,000.\n",
        "\n",
        "To apply Min-Max scaling to this dataset, we would first subtract the minimum value of each feature from all of its values. This would give us the following:\n",
        "\n",
        "Feature | Value\n",
        "------- | --------\n",
        "Sq. ft. | 0\n",
        "Bedrooms | 2\n",
        "Bathrooms | 1\n",
        "Price | 0\n",
        "We would then divide each value by the range of the feature. For the Sq. ft. feature, the range is 2000 - 1000 = 1000. So, we would divide each value in the Sq. ft. column by 1000. This would give us the following:\n",
        "\n",
        "- Feature | Value\n",
        "------- | --------\n",
        "Sq. ft. | 0.1\n",
        "Bedrooms | 2\n",
        "Bathrooms | 1\n",
        "Price | 0.5\n",
        "\n",
        "After Min-Max scaling, all of the features in the dataset have a range of 0 to 1. This makes them more comparable to each other, and it can help to improve the performance of machine learning algorithms.\n",
        "\n",
        "Here are some of the advantages of using Min-Max scaling:\n",
        "\n",
        "It is simple to understand and implement.\n",
        "It can be used with any type of data.\n",
        "It does not make any assumptions about the distribution of the data.\n",
        "Here are some of the disadvantages of using Min-Max scaling:\n",
        "\n",
        "It can amplify the effects of outliers.\n",
        "It can make the data less interpretable.\n"
      ],
      "metadata": {
        "id": "BiLm8nBWjMOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?  Provide an example to illustrate its application.\n",
        "Ans -\n",
        "The unit vector technique is a feature scaling technique that rescales each feature vector to have a unit length. This is done by dividing each feature vector by its Euclidean norm. The Euclidean norm is the length of the feature vector, which is calculated by taking the square root of the sum of the squares of all of the feature values.\n",
        "\n",
        "Unit vector scaling is different from Min-Max scaling in a few ways. First, unit vector scaling does not transform the data to a specific range, like 0 to 1. Instead, it simply ensures that each feature vector has a unit length. This means that the values of the features can still be in a wide range, but they will all be on the same scale.\n",
        "\n",
        "Second, unit vector scaling is less sensitive to outliers than Min-Max scaling. This is because outliers can have a large impact on the range of the data, which can distort the results of Min-Max scaling. Unit vector scaling is not affected by outliers, because it only considers the length of the feature vector, not the values of the individual features.\n",
        "\n",
        "To illustrate the application of unit vector scaling, consider the following dataset of house prices:\n",
        "\n",
        "Feature | Value\n",
        "------- | --------\n",
        "Sq. ft. | 1000\n",
        "Bedrooms | 3\n",
        "Bathrooms | 2\n",
        "Price | \\$500,000\n",
        "The Euclidean norm of the first feature vector is calculated as follows:\n",
        "\n",
        "Euclidean norm = sqrt(1000^2 + 3^2 + 2^2) = 100.5\n",
        "The first feature vector would then be divided by its Euclidean norm, giving us the following:\n",
        "\n",
        "Feature | Value\n",
        "------- | --------\n",
        "Sq. ft. | 0.1\n",
        "Bedrooms | 0.03\n",
        "Bathrooms | 0.02\n",
        "Price | 0.5\n",
        "We would repeat this process for the Bedrooms and Price features.\n",
        "\n",
        "After unit vector scaling, all of the feature vectors have a unit length. This makes them more comparable to each other, and it can help to improve the performance of machine learning algorithms that are sensitive to the scale of the features.\n",
        "\n",
        "Here are some of the advantages of using unit vector scaling:\n",
        "\n",
        "It is less sensitive to outliers than Min-Max scaling.\n",
        "It can help to improve the performance of machine learning algorithms that are sensitive to the scale of the features.\n",
        "Here are some of the disadvantages of using unit vector scaling:\n",
        "\n",
        "It does not transform the data to a specific range, which can make it difficult to interpret the results.\n",
        "It can be more computationally expensive than Min-Max scaling."
      ],
      "metadata": {
        "id": "NblToR-xjUoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature  Extraction? Provide an example to illustrate this concept.\n",
        "Ans -\n",
        "\n",
        "Principal component analysis (PCA) is a statistical technique that is used to reduce the dimensionality of a dataset while retaining as much information as possible. It does this by finding the directions of maximum variance in the data and projecting the data onto these directions.\n",
        "\n",
        "PCA can be used for a variety of purposes, including:\n",
        "\n",
        "Dimensionality reduction: PCA can be used to reduce the number of features in a dataset without losing too much information. This can be useful for machine learning algorithms that are not able to handle high-dimensional data.\n",
        "Feature extraction: PCA can be used to extract the most important features from a dataset. This can be useful for tasks such as data visualization and clustering.\n",
        "Noise reduction: PCA can be used to remove noise from a dataset. This can be useful for tasks such as image processing and signal processing.\n",
        "To illustrate the application of PCA, consider the following dataset of house prices:\n",
        "\n",
        "Feature | Value\n",
        "------- | --------\n",
        "Sq. ft. | 1000\n",
        "Bedrooms | 3\n",
        "Bathrooms | 2\n",
        "Price | \\$500,000\n",
        "This dataset has 3 features: Sq. ft., Bedrooms, and Bathrooms. We can use PCA to reduce the dimensionality of this dataset to 2 dimensions. PCA would first find the directions of maximum variance in the data. In this case, the two directions of maximum variance are the Sq. ft. and Bedrooms features. PCA would then project the data onto these two directions, giving us a new dataset with 2 features.\n",
        "\n",
        "The new dataset would still contain most of the information in the original dataset. However, it would be much easier to visualize and analyze.\n",
        "\n",
        "Here are some of the advantages of using PCA:\n",
        "\n",
        "It is a non-parametric technique, which means that it does not make any assumptions about the distribution of the data.\n",
        "It is relatively easy to understand and implement.\n",
        "It can be used with any type of data.\n",
        "Here are some of the disadvantages of using PCA:\n",
        "\n",
        "It can lose some information in the process of dimensionality reduction.\n",
        "It can be sensitive to outliers."
      ],
      "metadata": {
        "id": "juUfZoPkwBoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset  contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to  preprocess the data.\n",
        "Ans -\n"
      ],
      "metadata": {
        "id": "1HjJf7c-wUBS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xOBELW3si9S8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003603e7-9981-4d4d-baba-2dceaaa6476b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.66666667 0.5       ]\n",
            " [0.5        1.         1.        ]\n",
            " [1.         0.         0.        ]]\n",
            "[[ 1.5        -0.33333333  0.25      ]\n",
            " [ 2.          0.33333333  0.75      ]\n",
            " [ 2.5        -0.66666667 -0.25      ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the training data\n",
        "training_data = np.array([\n",
        "    [10, 4, 3],\n",
        "    [20, 5, 5],\n",
        "    [30, 2, 1],\n",
        "])\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the MinMaxScaler object to the training data\n",
        "scaler.fit(training_data)\n",
        "\n",
        "# Transform the training data\n",
        "scaled_training_data = scaler.transform(training_data)\n",
        "\n",
        "# Load the test data\n",
        "test_data = np.array([\n",
        "    [40, 1, 2],\n",
        "    [50, 3, 4],\n",
        "    [60, 0, 0],\n",
        "])\n",
        "\n",
        "# Transform the test data\n",
        "scaled_test_data = scaler.transform(test_data)\n",
        "\n",
        "print(scaled_training_data)\n",
        "print(scaled_test_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6. You are working on a project to build a model to predict stock prices. The dataset contains many  features, such as company financial data and market trends. Explain how you would use PCA to reduce the  dimensionality of the dataset."
      ],
      "metadata": {
        "id": "TQsazkI_xnbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the training data\n",
        "training_data = np.array([\n",
        "    [10, 4, 3],\n",
        "    [20, 5, 5],\n",
        "    [30, 2, 1],\n",
        "])\n",
        "\n",
        "# Create a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA object to the training data\n",
        "pca.fit(training_data)\n",
        "\n",
        "# Select the number of principal components to keep\n",
        "n_components = 2\n",
        "\n",
        "# Transform the training data using the PCA object\n",
        "scaled_training_data = pca.transform(training_data)\n",
        "\n",
        "# Transform the test data using the PCA object\n",
        "scaled_test_data = pca.transform(test_data)\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the training data\n",
        "training_data = np.array([\n",
        "    [170, 70, 25, 1, 120],\n",
        "    [160, 60, 30, 0, 110],\n",
        "    [150, 50, 40, 1, 100],\n",
        "])\n",
        "\n",
        "# Create a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA object to the training data\n",
        "pca.fit(training_data)\n",
        "\n",
        "# Calculate the cumulative explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Select the number of principal components to keep\n",
        "n_components = 2\n",
        "\n",
        "print(scaled_test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNDusr4sxZ1D",
        "outputId": "854e0f6e-404a-46f5-b7a6-459db78be0fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-9.92712861e+00 -1.25029143e+00  6.76096896e-17]\n",
            " [-3.44386144e-01  2.37890226e+00  2.22044605e-16]\n",
            " [ 1.02715148e+01 -1.12861083e+00  2.22044605e-16]]\n",
            "[[20.16285535  0.58356851 -1.10897395]\n",
            " [29.64329402  4.75843461 -0.27724349]\n",
            " [40.25919492  1.25092152 -0.27724349]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the  values to a range of -1 to 1.\n",
        "Ans -\n",
        "Here are the steps on how to perform Min-Max scaling to transform the values to a range of -1 to 1:\n",
        "\n",
        "Find the minimum and maximum values of the data:\n",
        "min_value = 1\n",
        "max_value = 20\n",
        "Subtract the minimum value from all of the values:\n",
        "scaled_data = data - min_value\n",
        "Divide all of the values by the range of the data:\n",
        "scaled_data = scaled_data / (max_value - min_value)\n",
        "Multiply all of the values by 2:\n",
        "scaled_data = scaled_data * 2\n",
        "Add 1 to all of the values:\n",
        "scaled_data = scaled_data + 1\n",
        "The resulting scaled data will be in the range of -1 to 1.\n",
        "\n",
        "Here is an example:\n",
        "\n",
        "data = [1, 5, 10, 15, 20]\n",
        "\n",
        "min_value = min(data)\n",
        "max_value = max(data)\n",
        "\n",
        "scaled_data = (data - min_value) / (max_value - min_value)\n",
        "scaled_data = scaled_data * 2\n",
        "scaled_data = scaled_data + 1\n",
        "\n",
        "print(scaled_data)\n",
        "[-1.  0.  1.  2.  3. ]\n",
        "The output shows the scaled data, which is now in the range of -1 to 1."
      ],
      "metadata": {
        "id": "RBoAqwU-yAcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform  Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
        "Ans -\n"
      ],
      "metadata": {
        "id": "T5BalsJyyNzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the training data\n",
        "training_data = np.array([\n",
        "    [170, 70, 25, 1, 120],\n",
        "    [160, 60, 30, 0, 110],\n",
        "    [150, 50, 40, 1, 100],\n",
        "])\n",
        "\n",
        "# Create a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA object to the training data\n",
        "pca.fit(training_data)\n",
        "\n",
        "# Calculate the cumulative explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Select the number of principal components to keep\n",
        "n_components = 2\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the training data\n",
        "training_data = np.array([\n",
        "    [170, 70, 25, 1, 120],\n",
        "    [160, 60, 30, 0, 110],\n",
        "    [150, 50, 40, 1, 100],\n",
        "])\n",
        "\n",
        "# Create a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA object to the training data\n",
        "pca.fit(training_data)\n",
        "\n",
        "# Calculate the cumulative explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Select the number of principal components to keep\n",
        "n_components = 2\n",
        "\n",
        "print(scaled_test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyjCG9h0yAAk",
        "outputId": "beea6422-f969-4237-d86e-d7c97085f715"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[20.16285535  0.58356851 -1.10897395]\n",
            " [29.64329402  4.75843461 -0.27724349]\n",
            " [40.25919492  1.25092152 -0.27724349]]\n"
          ]
        }
      ]
    }
  ]
}