{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650e2526",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier and Model Evaluation\n",
    "\n",
    "This notebook provides detailed explanations and examples for decision tree classification and classification model evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9452502",
   "metadata": {},
   "source": [
    "## Q1: Describe the Decision Tree Classifier Algorithm\n",
    "A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It is a tree-like model where each internal node represents a decision based on a feature, each branch represents an outcome, and each leaf node represents a class label.\n",
    "\n",
    "**How it works:**\n",
    "1. The dataset is split into subsets based on feature values.\n",
    "2. The splitting process continues recursively using criteria like Gini Impurity or Information Gain.\n",
    "3. The process stops when a stopping condition is met (e.g., maximum depth, minimum samples per leaf).\n",
    "4. To make predictions, the input data is traversed down the tree until a leaf node is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2085ef7",
   "metadata": {},
   "source": [
    "## Q2: Mathematical Intuition Behind Decision Tree Classification\n",
    "A Decision Tree splits data based on measures like Gini Impurity or Entropy. \n",
    "\n",
    "- **Entropy (Information Gain)**: Measures disorder in the dataset.\n",
    "  \\[ H(S) = - \\sum p_i \\log_2 p_i \\]\n",
    "  Information Gain is calculated as:\n",
    "  \\[ IG = H(parent) - \\sum \\frac{|S_{child}|}{|S_{parent}|} H(S_{child}) \\]\n",
    "\n",
    "- **Gini Impurity**: Measures impurity in the dataset.\n",
    "  \\[ Gini = 1 - \\sum p_i^2 \\]\n",
    "\n",
    "A decision tree selects the feature that maximizes Information Gain or minimizes Gini Impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e225e2",
   "metadata": {},
   "source": [
    "## Q3: Using Decision Tree for Binary Classification\n",
    "For a binary classification problem, a Decision Tree repeatedly splits the data into two groups until each subset is pure or meets a stopping criterion.\n",
    "\n",
    "### Example: Classifying Emails as Spam or Not Spam\n",
    "- Features: 'Contains word FREE?', 'Has attachment?', etc.\n",
    "- The tree splits based on these features to classify an email as spam or not spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e0914",
   "metadata": {},
   "source": [
    "## Q4: Geometric Intuition of Decision Trees\n",
    "A Decision Tree divides the feature space into rectangular regions.\n",
    "Each split creates a new decision boundary perpendicular to the feature axis.\n",
    "\n",
    "For example, a 2D dataset with two features (X, Y) will have axis-aligned decision boundaries, partitioning the space into different regions corresponding to class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07275ecf",
   "metadata": {},
   "source": [
    "## Q5: Confusion Matrix and Model Evaluation\n",
    "A **confusion matrix** is a table used to evaluate classification models. It has four values:\n",
    "\n",
    "| Actual \\ Predicted | Positive | Negative |\n",
    "|--------------------|----------|----------|\n",
    "| **Positive**      | TP       | FN       |\n",
    "| **Negative**      | FP       | TN       |\n",
    "\n",
    "- **True Positive (TP)**: Correctly predicted positive samples.\n",
    "- **False Negative (FN)**: Incorrectly predicted as negative.\n",
    "- **False Positive (FP)**: Incorrectly predicted as positive.\n",
    "- **True Negative (TN)**: Correctly predicted negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f2837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Example: True labels and predicted labels\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "cm, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6663af",
   "metadata": {},
   "source": [
    "## Q7: Importance of Choosing the Right Evaluation Metric\n",
    "Choosing the correct metric depends on the problem:\n",
    "- **Accuracy**: Good for balanced datasets.\n",
    "- **Precision**: Important when False Positives must be minimized (e.g., spam detection).\n",
    "- **Recall**: Important when False Negatives must be minimized (e.g., cancer detection).\n",
    "- **F1-score**: Useful when the dataset is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ec230",
   "metadata": {},
   "source": [
    "## Q8: Example Where Precision is Important\n",
    "**Example: Email Spam Detection**\n",
    "\n",
    "- If an email is mistakenly classified as spam (False Positive), an important email may be lost.\n",
    "- High precision ensures fewer false positives, avoiding misclassification of non-spam emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd8e88",
   "metadata": {},
   "source": [
    "## Q9: Example Where Recall is Important\n",
    "**Example: Medical Diagnosis for Cancer**\n",
    "\n",
    "- Missing a positive case (False Negative) means a person with cancer is not diagnosed.\n",
    "- High recall ensures that fewer actual positive cases are missed, even at the cost of some false positives."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}