{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqDpdHI5iQPn"
      },
      "outputs": [],
      "source": [
        "### Q1. Difference Between Linear Regression and Logistic Regression\n",
        "\n",
        "# Linear Regression:\n",
        "# - Used for predicting continuous dependent variables.\n",
        "# - Output is a continuous value.\n",
        "# - Fits a line to the data using the least squares method.\n",
        "# - Example: Predicting house prices based on area and location.\n",
        "\n",
        "# Logistic Regression:\n",
        "# - Used for predicting categorical dependent variables.\n",
        "# - Output is a probability that maps to a category using a sigmoid function.\n",
        "# - Example: Classifying emails as spam or not spam.\n",
        "\n",
        "### Q2. Cost Function in Logistic Regression\n",
        "\n",
        "# - The cost function used is the Log Loss or Binary Cross-Entropy.\n",
        "# - Formula: \\( J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\big[y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i))\\big] \\)\n",
        "# - Optimized using Gradient Descent to find the parameters that minimize the cost.\n",
        "\n",
        "### Q3. Regularization in Logistic Regression\n",
        "\n",
        "# - Regularization adds a penalty term to the cost function to prevent overfitting.\n",
        "# - Types:\n",
        "#   1. L1 Regularization (Lasso): Adds \\(\\lambda \\sum |\\theta_j|\\).\n",
        "#   2. L2 Regularization (Ridge): Adds \\(\\lambda \\sum \\theta_j^2\\).\n",
        "# - Helps by penalizing large coefficients, encouraging simpler models.\n",
        "\n",
        "### Q4. ROC Curve\n",
        "\n",
        "# - Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR).\n",
        "# - Area Under the Curve (AUC) measures the modelâ€™s ability to distinguish between classes.\n",
        "# - Ideal AUC is 1; random guessing yields an AUC of 0.5.\n",
        "\n",
        "### Q5. Feature Selection in Logistic Regression\n",
        "\n",
        "# Common techniques:\n",
        "# - Recursive Feature Elimination (RFE): Iteratively removes the least important features.\n",
        "# - L1 Regularization: Shrinks irrelevant feature coefficients to zero.\n",
        "# - Mutual Information: Measures the dependency between features and the target variable.\n",
        "# - Helps reduce model complexity and improve interpretability and performance.\n",
        "\n",
        "### Q6. Handling Imbalanced Datasets\n",
        "\n",
        "# Strategies:\n",
        "# - Resampling:\n",
        "#   1. Oversampling (e.g., SMOTE).\n",
        "#   2. Undersampling.\n",
        "# - Adjusting class weights: Assign higher weights to minority class samples.\n",
        "# - Using appropriate evaluation metrics: Precision, Recall, F1-Score, and AUC-ROC.\n",
        "\n",
        "### Q7. Common Issues and Challenges\n",
        "\n",
        "# - Multicollinearity:\n",
        "#   1. Use Variance Inflation Factor (VIF) to detect it.\n",
        "#   2. Drop correlated features or use PCA.\n",
        "# - Non-linearity:\n",
        "#   1. Logistic regression assumes linear relationships. Consider feature engineering or using non-linear models.\n",
        "# - Overfitting:\n",
        "#   1. Use regularization or cross-validation.\n",
        "# - Outliers:\n",
        "#   1. Detect using box plots or IQR and consider removing them.\n",
        "\n",
        "### Example Code:\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample Data\n",
        "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Splitting Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling Features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression\n",
        "model = LogisticRegression(penalty='l2', class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plotting ROC\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    }
  ]
}