{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
        "\n",
        "R-squared, also known as the coefficient of determination, is a statistical measure in linear regression that represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It indicates how well the regression model fits the data.\n",
        "\n",
        "Calculation of R-squared\n",
        "R-squared is calculated using the following formula:\n",
        "\n",
        "R^2 = 1 - (SS_res / SS_tot)\n",
        "\n",
        "Where:\n",
        "- SS_res is the sum of squares of residuals, calculated as:\n",
        "  SS_res = Œ£(y_i - ≈∑_i)^2\n",
        "  Here, y_i is the actual value, and ≈∑_i is the predicted value from the regression model.\n",
        "  \n",
        "- SS_tot is the total sum of squares, calculated as:\n",
        "  SS_tot = Œ£(y_i - »≥)^2\n",
        "  Here, »≥ is the mean of the actual values.\n",
        "\n",
        "Interpretation of R-squared\n",
        "- 0 ‚â§ R-squared ‚â§ 1: R-squared values range from 0 to 1.\n",
        "  - An R-squared of 0 means that the independent variables do not explain any of the variance in the dependent variable.\n",
        "  - An R-squared of 1 means that the independent variables explain all the variance in the dependent variable.\n",
        "\n",
        "- Higher R-squared values indicate a better fit of the model to the data, meaning that a higher proportion of the variance in the dependent variable is explained by the model.\n",
        "\n",
        "Considerations\n",
        "- Overfitting: A very high R-squared value may indicate overfitting, especially in models with many predictors relative to the number of observations. Overfitting occurs when the model captures the noise in the data rather than the underlying relationship.\n",
        "- Adjusted R-squared: To account for the number of predictors in the model, especially when comparing models with a different number of predictors, the adjusted R-squared is often used. It adjusts the R-squared value based on the number of predictors and the sample size, providing a more accurate measure of model fit.\n",
        "\n",
        "Limitations\n",
        "- Non-linear relationships: R-squared only measures the fit of linear models. It may be misleading in the presence of non-linear relationships between the dependent and independent variables.\n",
        "- Additional predictors: Adding more predictors to a model will always increase or maintain the R-squared value, even if the new predictors are not significant. This is why adjusted R-squared is preferred in such cases.\n",
        "\n",
        "In summary, R-squared is a valuable metric for assessing the goodness-of-fit in linear regression models, but it should be used with an understanding of its limitations and in conjunction with other metrics and diagnostics.\n"
      ],
      "metadata": {
        "id": "i5zHI_-9NVR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
        "\n",
        "Definition: Adjusted R-squared is a version of R-squared that accounts for the number of predictors in a regression model.\n",
        "\n",
        "Calculation:\n",
        "ùëÖ\n",
        "Àâ\n",
        "2\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "(\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùëÖ\n",
        "2\n",
        ")\n",
        "(\n",
        "ùëõ\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "ùëõ\n",
        "‚àí\n",
        "ùëò\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "R\n",
        "Àâ\n",
        "  \n",
        "2\n",
        " =1‚àí(\n",
        "n‚àík‚àí1\n",
        "(1‚àíR\n",
        "2\n",
        " )(n‚àí1)\n",
        "‚Äã\n",
        " )\n",
        "\n",
        "ùëÖ\n",
        "2\n",
        "R\n",
        "2\n",
        "  = Regular R-squared\n",
        "ùëõ\n",
        "n = Number of observations\n",
        "ùëò\n",
        "k = Number of predictors\n",
        "Key Differences:\n",
        "\n",
        "Penalty for Additional Predictors:\n",
        "\n",
        "R-squared: Increases with more predictors, even if they are insignificant.\n",
        "Adjusted R-squared: Penalizes the addition of non-significant predictors, increasing only if they improve the model.\n",
        "Model Fit:\n",
        "\n",
        "R-squared: Can be misleadingly high in overfitted models.\n",
        "Adjusted R-squared: Provides a more accurate fit by adjusting for the number of predictors.\n",
        "Comparison:\n",
        "\n",
        "R-squared: Not suitable for comparing models with different numbers of predictors.\n",
        "Adjusted R-squared: Better for comparison as it accounts for model complexity.\n"
      ],
      "metadata": {
        "id": "xF8o5TCqOlvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. When is it more appropriate to use adjusted R-squared?\n",
        "\n",
        "Adjusted R-squared is more appropriate to use than regular R-squared when comparing regression models that have different numbers of predictors (independent variables). The adjusted R-squared adjusts for the number of predictors in the model, penalizing the addition of unnecessary predictors. This helps to provide a more accurate measure of how well the model generalizes to new data.\n",
        "\n",
        "Specifically, use adjusted R-squared when:\n",
        "\n",
        "Comparing Models: You want to compare models with different numbers of predictors.\n",
        "Avoiding Overfitting: You are concerned about overfitting and want a measure that considers the complexity of the model.\n",
        "Model Selection: You are selecting the best model among a set of models with varying predictors."
      ],
      "metadata": {
        "id": "JB5x4YR3cnTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
        "\n",
        "Mean Squared Error (MSE):\n",
        "\n",
        "Definition: Average of the squared differences between actual and predicted values.\n",
        "Formula: MSE = (1/n) * Œ£(y_i - ≈∑_i)¬≤\n",
        "Represents: The overall error, heavily penalizing larger errors.\n",
        "2. Root Mean Squared Error (RMSE):\n",
        "\n",
        "Definition: Square root of the MSE.\n",
        "Formula: RMSE = ‚àöMSE = ‚àö((1/n) * Œ£(y_i - ≈∑_i)¬≤)\n",
        "Represents: The standard deviation of prediction errors, in the same units as the target variable.\n",
        "3. Mean Absolute Error (MAE):\n",
        "\n",
        "Definition: Average of the absolute differences between actual and predicted values.\n",
        "Formula: MAE = (1/n) * Œ£|y_i - ≈∑_i|\n",
        "Represents: The average magnitude of errors, less sensitive to outliers."
      ],
      "metadata": {
        "id": "tKNovZ65cr3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
        "\n",
        "\n",
        "Mean Squared Error (MSE):\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Sensitivity to Large Errors: MSE penalizes larger errors more than smaller ones due to squaring, which can be useful in scenarios where large errors are particularly undesirable.\n",
        "Mathematical Properties: The squaring operation makes the derivative easier to compute, which is beneficial for optimization algorithms used in training models.\n",
        "Disadvantages:\n",
        "\n",
        "Sensitivity to Outliers: MSE can be overly sensitive to outliers since it squares the error, amplifying the effect of extreme values.\n",
        "Interpretability: The units of MSE are the square of the units of the target variable, making it less interpretable in practical terms.\n",
        "2. Root Mean Squared Error (RMSE):\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Interpretability: RMSE is in the same units as the target variable, making it more interpretable and easier to relate to the actual data.\n",
        "Sensitivity to Large Errors: Like MSE, RMSE penalizes larger errors, which can be useful if large errors are particularly problematic.\n",
        "Disadvantages:\n",
        "\n",
        "Sensitivity to Outliers: RMSE shares the same sensitivity to outliers as MSE due to the squaring of errors.\n",
        "Complexity: It is more complex to compute than MAE, and the square root operation adds an extra computational step.\n",
        "3. Mean Absolute Error (MAE):\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Robustness to Outliers: MAE is less sensitive to outliers compared to MSE and RMSE since it does not square the error terms.\n",
        "Interpretability: MAE is in the same units as the target variable and provides a straightforward measure of average prediction error.\n",
        "Disadvantages:\n",
        "\n",
        "Equal Weight to All Errors: MAE gives equal weight to all errors, which might not be ideal if larger errors need to be penalized more.\n",
        "Optimization Complexity: The absolute value operation can be less convenient for mathematical optimization compared to squaring, making it less preferred for some optimization algorithms.\n",
        "\n"
      ],
      "metadata": {
        "id": "5MhRaBeSdSsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
        "\n",
        "Lasso Regularization (Least Absolute Shrinkage and Selection Operator):\n",
        "\n",
        "Concept: Lasso regularization is a technique used to prevent overfitting in regression models by adding a penalty equivalent to the absolute value of the magnitude of coefficients.\n",
        "Formula:\n",
        "Lasso¬†Loss\n",
        "=\n",
        "RSS\n",
        "+\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëù\n",
        "‚à£\n",
        "ùõΩ\n",
        "ùëó\n",
        "‚à£\n",
        "Lasso¬†Loss=RSS+Œª\n",
        "j=1\n",
        "‚àë\n",
        "p\n",
        "‚Äã\n",
        " ‚à£Œ≤\n",
        "j\n",
        "‚Äã\n",
        " ‚à£\n",
        "\n",
        "where\n",
        "RSS\n",
        "RSS is the residual sum of squares,\n",
        "ùúÜ\n",
        "Œª is the regularization parameter, and\n",
        "ùõΩ\n",
        "ùëó\n",
        "Œ≤\n",
        "j\n",
        "‚Äã\n",
        "  are the coefficients.\n",
        "Effect: The Lasso penalty can shrink some coefficients to exactly zero, effectively performing variable selection by removing less important predictors from the model.\n",
        "Ridge Regularization:\n",
        "\n",
        "Concept: Ridge regularization is another technique used to prevent overfitting by adding a penalty equivalent to the square of the magnitude of coefficients.\n",
        "Formula:\n",
        "Ridge¬†Loss\n",
        "=\n",
        "RSS\n",
        "+\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëù\n",
        "ùõΩ\n",
        "ùëó\n",
        "2\n",
        "Ridge¬†Loss=RSS+Œª\n",
        "j=1\n",
        "‚àë\n",
        "p\n",
        "‚Äã\n",
        " Œ≤\n",
        "j\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "\n",
        "where\n",
        "ùúÜ\n",
        "Œª is the regularization parameter, and\n",
        "ùõΩ\n",
        "ùëó\n",
        "Œ≤\n",
        "j\n",
        "‚Äã\n",
        "  are the coefficients.\n",
        "Effect: Ridge penalty shrinks the coefficients towards zero but never sets them exactly to zero, retaining all predictors but reducing their impact.\n",
        "Differences Between Lasso and Ridge:\n",
        "\n",
        "Penalty Term: Lasso uses the absolute value (\n",
        "ùêø\n",
        "1\n",
        "L1 norm) of coefficients, while Ridge uses the square (\n",
        "ùêø\n",
        "2\n",
        "L2 norm) of coefficients.\n",
        "Variable Selection: Lasso can perform variable selection by shrinking some coefficients to zero, thus eliminating some predictors. Ridge shrinks coefficients but retains all predictors.\n",
        "Use Cases:\n",
        "Lasso: Preferred when you want a simpler model that performs feature selection, especially useful when you suspect that many predictors are irrelevant or redundant.\n",
        "Ridge: Preferred when you believe all predictors contribute to the outcome and want to shrink their influence without eliminating any."
      ],
      "metadata": {
        "id": "P5vYEDNqdlxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
        "\n",
        "Concept of Overfitting:\n",
        "\n",
        "Overfitting occurs when a model learns not only the underlying patterns in the training data but also the noise and outliers. This leads to poor generalization to new, unseen data.\n",
        "How Regularized Linear Models Help:\n",
        "\n",
        "Regularized linear models add a penalty to the loss function, which constrains the magnitude of the coefficients. This prevents the model from fitting the noise and outliers too closely, thus improving generalization to new data.\n",
        "Types of Regularization:\n",
        "\n",
        "Ridge Regularization (L2 penalty): Adds a penalty equivalent to the sum of the squares of the coefficients.\n",
        "Lasso Regularization (L1 penalty): Adds a penalty equivalent to the sum of the absolute values of the coefficients.\n",
        "Elastic Net: Combines both L1 and L2 penalties.\n",
        "Example to Illustrate Regularization\n",
        "Scenario:\n",
        "Suppose you have a dataset with 100 predictors (features) and a target variable. You want to build a linear regression model to predict the target variable.\n",
        "\n",
        "Without Regularization:\n",
        "\n",
        "Fit a linear regression model using all 100 predictors.\n",
        "The model has high variance, fitting the training data almost perfectly.\n",
        "When tested on new data, the model performs poorly because it has learned the noise in the training data (overfitting).\n",
        "With Regularization:\n",
        "\n",
        "Ridge Regularization:\n",
        "\n",
        "Fit a linear regression model with an L2 penalty.\n",
        "The penalty term shrinks the coefficients of less important predictors, reducing their impact.\n",
        "The model focuses more on the most significant predictors, avoiding overfitting by not fitting the noise.\n",
        "Ridge¬†Loss\n",
        "=\n",
        "RSS\n",
        "+\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "100\n",
        "ùõΩ\n",
        "ùëó\n",
        "2\n",
        "Ridge¬†Loss=RSS+Œª\n",
        "j=1\n",
        "‚àë\n",
        "100\n",
        "‚Äã\n",
        " Œ≤\n",
        "j\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "Lasso Regularization:\n",
        "\n",
        "Fit a linear regression model with an L1 penalty.\n",
        "The penalty term can shrink some coefficients to exactly zero, effectively selecting a subset of the predictors.\n",
        "The model becomes simpler and less likely to overfit, as it only uses the most relevant predictors.\n",
        "Lasso¬†Loss\n",
        "=\n",
        "RSS\n",
        "+\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "100\n",
        "‚à£\n",
        "ùõΩ\n",
        "ùëó\n",
        "‚à£\n",
        "Lasso¬†Loss=RSS+Œª\n",
        "j=1\n",
        "‚àë\n",
        "100\n",
        "‚Äã\n",
        " ‚à£Œ≤\n",
        "j\n",
        "‚Äã\n",
        " ‚à£\n",
        "Illustrative Example:\n",
        "Consider a toy dataset with the following properties:\n",
        "\n",
        "Predictors (features):\n",
        "ùëã\n",
        "1\n",
        ",\n",
        "ùëã\n",
        "2\n",
        ",\n",
        "‚Ä¶\n",
        ",\n",
        "ùëã\n",
        "100\n",
        "X\n",
        "1\n",
        "‚Äã\n",
        " ,X\n",
        "2\n",
        "‚Äã\n",
        " ,‚Ä¶,X\n",
        "100\n",
        "‚Äã\n",
        "\n",
        "Target variable:\n",
        "ùëå\n",
        "Y"
      ],
      "metadata": {
        "id": "STpTT5sbeNfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "X = np.random.rand(100, 100)\n",
        "true_coefs = np.random.rand(100)\n",
        "Y = X.dot(true_coefs) + np.random.normal(0, 0.1, 100)\n",
        "\n",
        "# Split the data\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "\n",
        "#Fit models\n",
        "lr = LinearRegression().fit(X_train,Y_train)\n",
        "ridge = Ridge(alpha=1.0).fit(X_train, Y_train)\n",
        "lasso  = Lasso(alpha=0.1).fit(X_train, Y_train)\n",
        "\n",
        "# Predictions\n",
        "Y_pred_lr = lr.predict(X_test)\n",
        "Y_pred_ridge = ridge.predict(X_test)\n",
        "Y_pred_lasso = lasso.predict(X_test)\n",
        "\n",
        "# Calculate MSE\n",
        "mse_lr = mean_squared_error(Y_test, Y_pred_lr)\n",
        "mse_ridge = mean_squared_error(Y_test, Y_pred_ridge)\n",
        "mse_lasso = mean_squared_error(Y_test, Y_pred_lasso)\n",
        "\n",
        "print(\"MSE without regularization:\", mse_lr)\n",
        "print(\"MSE with Ridge reqularization:\", mse_ridge)\n",
        "print(\"MSE with Lasso regularization\", mse_lasso)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ8MXpLiemyg",
        "outputId": "1a020e14-9a92-47c0-9253-72446d2e0ad0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE without regularization: 0.3084888044660998\n",
            "MSE with Ridge reqularization: 0.37410684027368707\n",
            "MSE with Lasso regularization 1.9867859890389563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
        "\n",
        "Assumption of Linearity:\n",
        "\n",
        "Limitation: Assumes a linear relationship.\n",
        "Impact: Poor performance on nonlinear data.\n",
        "2. Interpretation Challenges:\n",
        "\n",
        "Limitation: Shrinks coefficients, complicating interpretation.\n",
        "Impact: Difficult to understand individual predictor effects.\n",
        "3. Hyperparameter Tuning:\n",
        "\n",
        "Limitation: Requires tuning the regularization parameter (\n",
        "ùúÜ\n",
        "Œª).\n",
        "Impact: Adds complexity and computational cost.\n",
        "4. Variable Selection Issues (Lasso):\n",
        "\n",
        "Limitation: Unstable with correlated predictors.\n",
        "Impact: Arbitrary exclusion of relevant variables.\n",
        "5. No Variable Selection (Ridge):\n",
        "\n",
        "Limitation: Retains all predictors.\n",
        "Impact: May include irrelevant features in high-dimensional data.\n",
        "6. Bias-Variance Trade-off:\n",
        "\n",
        "Limitation: Introduces bias by shrinking coefficients.\n",
        "Impact: Risk of underfitting if\n",
        "ùúÜ\n",
        "Œª is too large.\n",
        "7. Data Standardization Requirement:\n",
        "\n",
        "Limitation: Requires standardization.\n",
        "Impact: Adds preprocessing complexity.\n",
        "Situations Where They May Not Be Best\n",
        "Nonlinear Relationships: Use models like decision trees or neural networks.\n",
        "Complex Interactions: Use ensemble methods or deep learning.\n",
        "Interpretability Needs: Prefer simpler, unregularized models.\n",
        "Multicollinearity: Use methods like PCR or PLS."
      ],
      "metadata": {
        "id": "2V6C5xONoqaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
        "\n",
        "\n",
        "\n",
        "Given:\n",
        "\n",
        "Model A has an RMSE of 10.\n",
        "Model B has an MAE of 8.\n",
        "Choosing the Better Performer:\n",
        "Understanding RMSE and MAE:\n",
        "\n",
        "RMSE (Root Mean Squared Error): Measures the square root of the average squared differences between actual and predicted values. It penalizes larger errors more heavily.\n",
        "MAE (Mean Absolute Error): Measures the average absolute differences between actual and predicted values. It treats all errors equally, making it less sensitive to outliers.\n",
        "Comparison:\n",
        "Model A (RMSE = 10):\n",
        "\n",
        "RMSE emphasizes larger errors, meaning this model might be better at avoiding significant deviations in predictions.\n",
        "A lower RMSE indicates fewer large errors, but it can be misleading if a few large errors heavily influence the metric.\n",
        "Model B (MAE = 8):\n",
        "\n",
        "MAE provides a straightforward measure of the average prediction error.\n",
        "A lower MAE suggests better overall prediction accuracy, but it doesn't penalize large errors as much as RMSE.\n",
        "Decision Factors:\n",
        "Impact of Large Errors:\n",
        "\n",
        "If large errors are particularly undesirable in your application (e.g., predicting financial losses, safety-critical predictions), the RMSE might be more relevant.\n",
        "In this scenario, Model A (RMSE = 10) might be preferred if you want to minimize the impact of large errors.\n",
        "Overall Accuracy:\n",
        "\n",
        "If you care more about the average error and can tolerate occasional large errors, MAE might be more appropriate.\n",
        "In this scenario, Model B (MAE = 8) might be preferred for its lower average error.\n",
        "Limitations of the Metrics:\n",
        "RMSE:\n",
        "\n",
        "Sensitive to Outliers: RMSE can be heavily influenced by a few large errors, which might not represent typical model performance.\n",
        "Interpretability: RMSE is in the same units as the target variable but less intuitive compared to MAE.\n",
        "MAE:\n",
        "\n",
        "Equal Weight to All Errors: MAE treats all errors equally, which might not be suitable if large errors are critical.\n",
        "Less Sensitivity to Large Errors: MAE might under-represent the impact of large errors.\n",
        "Conclusion:\n",
        "Given the provided information, Model B (MAE = 8) would generally be considered the better performer due to its lower average error, indicating better overall accuracy. However, this choice depends on the specific context and tolerance for large errors in your application.\n",
        "\n",
        "Recommendations:\n",
        "Calculate Both Metrics for Both Models:\n",
        "Ideally, you should have both RMSE and MAE for both models to make a comprehensive comparison.\n",
        "Contextual Considerations:\n",
        "Consider the specific needs of your application. If large errors are critical, further investigate Model A's MAE or Model B's RMSE.\n",
        "Complementary Metrics:\n",
        "Use other complementary metrics like\n",
        "ùëÖ\n",
        "2\n",
        "R\n",
        "2\n",
        "  (coefficient of determination) or residual plots to get a fuller picture of model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "M_-S0QtopDCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Given:\n",
        "\n",
        "Model A: Ridge regularization with\n",
        "ùúÜ\n",
        "=\n",
        "0.1\n",
        "Œª=0.1\n",
        "Model B: Lasso regularization with\n",
        "ùúÜ\n",
        "=\n",
        "0.5\n",
        "Œª=0.5\n",
        "Choosing the Better Performer:\n",
        "1. Understanding Ridge and Lasso Regularization:\n",
        "\n",
        "Ridge Regularization (L2): Adds a penalty proportional to the sum of the squared coefficients. This penalty term shrinks the coefficients but does not set them to zero.\n",
        "Ridge¬†Loss\n",
        "=\n",
        "RSS\n",
        "+\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëù\n",
        "ùõΩ\n",
        "ùëó\n",
        "2\n",
        "Ridge¬†Loss=RSS+Œª\n",
        "j=1\n",
        "‚àë\n",
        "p\n",
        "‚Äã\n",
        " Œ≤\n",
        "j\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "Lasso Regularization (L1): Adds a penalty proportional to the sum of the absolute values of the coefficients. This penalty can shrink some coefficients to zero, effectively performing feature selection.\n",
        "Lasso¬†Loss\n",
        "=\n",
        "RSS\n",
        "+\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëù\n",
        "‚à£\n",
        "ùõΩ\n",
        "ùëó\n",
        "‚à£\n",
        "Lasso¬†Loss=RSS+Œª\n",
        "j=1\n",
        "‚àë\n",
        "p\n",
        "‚Äã\n",
        " ‚à£Œ≤\n",
        "j\n",
        "‚Äã\n",
        " ‚à£\n",
        "Decision Factors:\n",
        "Feature Selection:\n",
        "\n",
        "Lasso (Model B): If your goal is to perform feature selection, Lasso is preferable as it can shrink some coefficients to zero, effectively reducing the number of predictors.\n",
        "Ridge (Model A): If you believe all predictors are relevant and you want to shrink their coefficients without eliminating any, Ridge is preferable.\n",
        "Handling Multicollinearity:\n",
        "\n",
        "Ridge (Model A): Ridge is better at handling multicollinearity (highly correlated predictors) as it distributes the coefficient weights among correlated features.\n",
        "Lasso (Model B): Lasso might arbitrarily select one among correlated features, potentially excluding some important ones.\n",
        "Model Interpretability:\n",
        "\n",
        "Lasso (Model B): Provides a simpler model with potentially fewer predictors, making it easier to interpret.\n",
        "Ridge (Model A): Retains all predictors, which might make the model more complex and harder to interpret.\n",
        "Regularization Parameter (\n",
        "ùúÜ\n",
        "Œª):\n",
        "\n",
        "Different Values: The given\n",
        "ùúÜ\n",
        "Œª values are different (\n",
        "ùúÜ\n",
        "=\n",
        "0.1\n",
        "Œª=0.1 for Ridge and\n",
        "ùúÜ\n",
        "=\n",
        "0.5\n",
        "Œª=0.5 for Lasso), making a direct comparison challenging. The choice of\n",
        "ùúÜ\n",
        "Œª significantly impacts the regularization strength.\n",
        "Trade-offs and Limitations:\n",
        "Trade-offs:\n",
        "\n",
        "Bias-Variance Trade-off: Both regularization methods introduce bias to reduce variance, but they do so differently. Ridge reduces the impact of all predictors, while Lasso can exclude some predictors entirely.\n",
        "Model Complexity vs. Performance: Ridge tends to result in more complex models with all predictors, while Lasso may produce simpler models with fewer predictors.\n",
        "Limitations:\n",
        "\n",
        "Ridge: Does not perform feature selection, which might be a limitation if you aim to simplify the model by excluding irrelevant features.\n",
        "Lasso: Can be unstable with correlated predictors and might exclude relevant features arbitrarily."
      ],
      "metadata": {
        "id": "nOOx9cObxUMB"
      }
    }
  ]
}